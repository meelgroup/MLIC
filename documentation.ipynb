{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Learn Boolean decision rules\n",
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyrulelearn.imli import imli\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Configuration\n",
    "\n",
    "We now create an instance of `imli` object. First we learn a classification rule in CNF, that is, the decision rule is ANDs of ORs of input features. For that, we specify `rule_type=CNF` inside the model. In this example, we learn a 2 clause rule with parameter `data_fidelity=10`. `data_fidelity` parameter sets the priority between accuracy and rule-sparsity such that a higher value of `data_fidelity` results in a more accurate rule. We require a MaxSAT solver to learn the Boolean rule. In this example, we use `open-wbo` as the MaxSAT solver. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = imli(rule_type=\"CNF\", num_clause=2,  data_fidelity=10, solver=\"open-wbo\", work_dir=\"pyrulelearn/\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "In this example, we learn a decision rule on `Iris` flower dataset. While the original dataset is used for multiclass classification, we modify it for binary classification. Our objective is to learn a decision rule that separates `Iris Versicolour` from other two classes of Iris: `Iris Setosa` and `Iris Virginica`. \n",
    "\n",
    "Our framework requires the training set to be discretized. In the following, we apply entropy-based discretization on the dataset. Alternatively, one can discreize the dataset and directly use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Applying entropy based discretization using Orange library\n- file name:  benchmarks/iris_orange.csv\n- the number of discretized features: 11\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(['sepal length <  5.45',\n",
       "  'sepal length = (5.45 - 7.05)',\n",
       "  'sepal length >=  7.05',\n",
       "  'sepal width <  2.95',\n",
       "  'sepal width >=  2.95',\n",
       "  'petal length <  2.45',\n",
       "  'petal length = (2.45 - 4.75)',\n",
       "  'petal length >=  4.75',\n",
       "  'petal width <  0.8',\n",
       "  'petal width = (0.8 - 1.75)',\n",
       "  'petal width >=  1.75'],\n",
       " array([[1., 0., 0., ..., 1., 0., 0.],\n",
       "        [1., 0., 0., ..., 1., 0., 0.],\n",
       "        [1., 0., 0., ..., 1., 0., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0., ..., 0., 0., 1.],\n",
       "        [0., 1., 0., ..., 0., 0., 1.],\n",
       "        [0., 1., 0., ..., 0., 0., 1.]]))"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "X, y, features = model.discretize_orange(\"benchmarks/iris_orange.csv\")\n",
    "features, X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nTraining started for batch:  1\n- number of soft clauses:  93\n- number of Boolean variables: 157\n- number of hard and soft clauses: 863\n\n\nBatch tarining complete\n- number of literals in the rule: 2\n- number of training errors:    3 out of 49\n\nTraining started for batch:  2\n- number of soft clauses:  95\n- number of Boolean variables: 161\n- number of hard and soft clauses: 890\n\n\nBatch tarining complete\n- number of literals in the rule: 4\n- number of training errors:    1 out of 51\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report performance of the learned rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training report: \n              precision    recall  f1-score   support\n\n           0       0.98      0.95      0.97        65\n           1       0.92      0.97      0.94        35\n\n    accuracy                           0.96       100\n   macro avg       0.95      0.96      0.96       100\nweighted avg       0.96      0.96      0.96       100\n\n\ntest report: \n              precision    recall  f1-score   support\n\n           0       1.00      0.97      0.99        35\n           1       0.94      1.00      0.97        15\n\n    accuracy                           0.98        50\n   macro avg       0.97      0.99      0.98        50\nweighted avg       0.98      0.98      0.98        50\n\n"
     ]
    }
   ],
   "source": [
    "print(\"training report: \")\n",
    "print(classification_report(y_train, model.predict(X_train), target_names=['0','1']))\n",
    "print()\n",
    "print(\"test report: \")\n",
    "print(classification_report(y_test, model.predict(X_test), target_names=['0','1']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the learned rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learned rule is: \n\nAn Iris flower is predicted as Iris Versicolor if\n( sepal length = (5.45 - 7.05) OR petal length = (2.45 - 4.75) ) AND \n( petal length = (2.45 - 4.75) OR petal width = (0.8 - 1.75))\n"
     ]
    }
   ],
   "source": [
    "rule = model.get_rule(features)\n",
    "print(\"Learned rule is: \\n\")\n",
    "print(\"An Iris flower is predicted as Iris Versicolor if\")\n",
    "print(rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Learn decision rules as DNF\n",
    "\n",
    "To learn a decision rule as a DNF (ORs of ANDs of input features), we specify `rule_type=DNF` in the parameters of the model. In the following, we learn a 2 clause DNF decision rule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = imli(rule_type=\"DNF\", num_clause=2,  data_fidelity=10, solver=\"open-wbo\", work_dir=\"pyrulelearn/\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training report: \n              precision    recall  f1-score   support\n\n           0       0.93      0.98      0.96        65\n           1       0.97      0.86      0.91        35\n\n    accuracy                           0.94       100\n   macro avg       0.95      0.92      0.93       100\nweighted avg       0.94      0.94      0.94       100\n\n\ntest report: \n              precision    recall  f1-score   support\n\n           0       0.97      1.00      0.99        35\n           1       1.00      0.93      0.97        15\n\n    accuracy                           0.98        50\n   macro avg       0.99      0.97      0.98        50\nweighted avg       0.98      0.98      0.98        50\n\n\n( not sepal length >=  7.05 AND petal width = (0.8 - 1.75) ) OR \n( petal length = (2.45 - 4.75))\n\n[[-2, 9], [6]]\n"
     ]
    }
   ],
   "source": [
    "X, y, features = model.discretize_orange(\"benchmarks/iris_orange.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "model.fit(X_train,y_train)\n",
    "print(\"training report: \")\n",
    "print(classification_report(y_train, model.predict(X_train), target_names=['0','1']))\n",
    "print()\n",
    "print(\"test report: \")\n",
    "print(classification_report(y_test, model.predict(X_test), target_names=['0','1']))\n",
    "\n",
    "print()\n",
    "print(model.get_rule(features))\n",
    "print()\n",
    "print(model.get_selected_column_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Learn more expressible decision rules\n",
    "\n",
    "Our framework allows one to learn more expressible decision rules, which we call relaxed_CNF rules. This rule allows thresholds on satisfaction of clauses and literals and can learn more complex decision boundaries. See the [ECAI-2020](https://bishwamittra.github.io/publication/ecai_2020/paper.pdf) paper for more details. \n",
    "\n",
    "\n",
    "In our framework, set the parameter `rule_type=relaxed_CNF` to learn the rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = imli(rule_type=\"relaxed_CNF\", num_clause=2,  data_fidelity=10, solver=\"cplex\", work_dir=\"pyrulelearn/\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "training report: \n              precision    recall  f1-score   support\n\n           0       0.96      0.98      0.97        65\n           1       0.97      0.91      0.94        35\n\n    accuracy                           0.96       100\n   macro avg       0.96      0.95      0.96       100\nweighted avg       0.96      0.96      0.96       100\n\n\ntest report: \n              precision    recall  f1-score   support\n\n           0       0.97      1.00      0.99        35\n           1       1.00      0.93      0.97        15\n\n    accuracy                           0.98        50\n   macro avg       0.99      0.97      0.98        50\nweighted avg       0.98      0.98      0.98        50\n\n"
     ]
    }
   ],
   "source": [
    "X, y, features = model.discretize_orange(\"benchmarks/iris_orange.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "model.fit(X_train,y_train)\n",
    "print(\"training report: \")\n",
    "print(classification_report(y_train, model.predict(X_train), target_names=['0','1']))\n",
    "print()\n",
    "print(\"test report: \")\n",
    "print(classification_report(y_test, model.predict(X_test), target_names=['0','1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the decision rule\n",
    "\n",
    "In this example, we ask the framework to learn a 2 clause rule. During training, we learn the thresholds on clauses and literals while fitting the dataset. The learned rule operates in two levels. In the first level, a clause is satisfied if the literals in the clause satisfy the learned threshold on literals. In the second level, the formula is satisfied when the threshold on clauses is satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Learned rule is: \n\nAn Iris flower is predicted as Iris Versicolor if\n[ (  sepal width >=  2.95  + petal length = (2.45 - 4.75)  + petal width = (0.8 - 1.75)  + not sepal length >=  7.05   )>= 3  ] +\n[ ( )>= 0  ]  >= 2\n\nThrehosld on clause: 2\nThreshold on literals: (this is a list where the entries denote threholds on literals on all clauses)\n[3, 0]\n"
     ]
    }
   ],
   "source": [
    "rule = model.get_rule(features)\n",
    "print(\"Learned rule is: \\n\")\n",
    "print(\"An Iris flower is predicted as Iris Versicolor if\")\n",
    "print(rule)\n",
    "print(\"\\nThrehosld on clause:\", model.get_threshold_clause())\n",
    "print(\"Threshold on literals: (this is a list where the entries denote threholds on literals on all clauses)\")\n",
    "print(model.get_threshold_literal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}